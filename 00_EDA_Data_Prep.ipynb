{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstate the EDA and prepare the data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis/Expectations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "#Data profiling/EDA \n",
    "import pandas_profiling \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 500) # default is 60 rows\n",
    "pd.set_option('max_columns', 500) # default is 20 columns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing custom library functions\n",
    "# check at https://github.com/ved93/neo\n",
    "from neo import visualize, eda, feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get plotly type plot using df.plot method \n",
    "#https://plotly.com/python/pandas-backend/\n",
    "# pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/vedprakash/Downloads/DS Test/Training/X_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'clf' #'reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the target var\n",
    "df['target'] = df['n3']\n",
    "df.drop(['n3'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the target var\n",
    "if len(df.target.unique()) < 5:\n",
    "    temp = df.target.value_counts()\n",
    "\n",
    "    dftemp = pd.DataFrame({'labels': temp.index,\n",
    "                       'values': temp.values\n",
    "                      })\n",
    "\n",
    "\n",
    "    # fig = px.pie(dftemp, values='values', title='Class Distribution',labels='labels')\n",
    "    fig = dftemp.iplot(kind='pie',labels='labels',values='values', title='Target Class Distribution',\n",
    "                  dimensions = (400,300)\n",
    "                  )    \n",
    "else:\n",
    "    #try this if error -- , kde_kws={'bw': 0.1}\n",
    "#     visualize.density_plot(df , 'target' )\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(range(df.shape[0]), np.sort(df.target.values))\n",
    "    plt.xlabel('index', fontsize=12)\n",
    "    plt.ylabel('target', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    #if there are outliers then do followiung\n",
    "    ulimit = np.percentile(df.target.values, 99)\n",
    "    llimit = np.percentile(df.target.values, 1)\n",
    "    df['target'].loc[df['target']>ulimit] = ulimit\n",
    "    df['target'].loc[df['target']<llimit] = llimit\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.distplot(df.target.values, bins=50, kde=False)\n",
    "    plt.xlabel('target', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.distplot(df[\"target\"].values, bins=50, kde=False)\n",
    "    # sns.distplot(dfpro.req_next_30.values, hist_kws={'alpha': 0.9}, kde=False)\n",
    "    plt.xlabel('Target', fontsize=12)\n",
    "    plt.title(\"Target Histogram\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    #log of target var\n",
    "    #reference -https://www.kaggle.com/sudalairajkumar/simple-exploration-baseline-santander-value\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.distplot( np.log1p(df[\"target\"].values), bins=50, kde=False)\n",
    "    plt.xlabel('Target', fontsize=12)\n",
    "    plt.title(\"Log of Target Histogram\", fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    eda.print_quantiles(df, 'target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda.gen_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eda.general_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.create_summary_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfmissing=check_missing_data(df)\n",
    "\n",
    "# dfmissing\n",
    "\n",
    "# col_high_missing = [col for col in dfmissing.columns if dfmissing.loc['Percent',col] > 90 ]\n",
    "\n",
    "# len(col_high_missing)\n",
    "\n",
    "# # We will remove variables with 90% missing values. This is fair assumption to remove but we will revisit it again if we dont see any performance. These missing values might contains some info\n",
    "\n",
    "# dfnonmissing = df.loc[:,~df.columns.isin(col_high_missing)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Observations from Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols =list(col for col in df.select_dtypes('object').columns if col not in ['appointmentId'])\n",
    "num_cols = list(col for col in df.select_dtypes(['int64','float64']).columns if col not in ['appointmentId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'reg':\n",
    "    # lets check correlation\n",
    "\n",
    "    from scipy.stats import spearmanr\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "    #let us first take the 'float' variables alone\n",
    "    # and then get the correlation with the target variable to see how they are related.\n",
    "    # Let us just impute the missing values with mean values to compute correlation coefficients #\n",
    "    mean_values = df.mean(axis=0)\n",
    "    train_df_new = df.fillna(mean_values )\n",
    "\n",
    "    # Now let us look at the correlation coefficient of each of these variables #\n",
    "    #logerror is the target var\n",
    "    x_cols = [col for col in train_df_new.columns if col not in ['target','unique_id'] if 'n' in col]\n",
    "\n",
    "\n",
    "    labels = []\n",
    "    values = []\n",
    "    for col in x_cols:\n",
    "        labels.append(col)\n",
    "        values.append(np.corrcoef(train_df_new[col].values, train_df_new.target.values)[0,1])\n",
    "    corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n",
    "    corr_df = corr_df.sort_values(by='corr_values')\n",
    "    print(corr_df)\n",
    "\n",
    "    ind = np.arange(len(labels))\n",
    "    width = 0.9\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\n",
    "    ax.set_yticks(ind)\n",
    "    ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n",
    "    ax.set_xlabel(\"Correlation coefficient\")\n",
    "    ax.set_title(\"Correlation coefficient of the variables\")\n",
    "    #autolabel(rects)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check zero corr coef var\n",
    "corr_zero_cols = ['N9','N2','N17','N29']\n",
    "for col in corr_zero_cols:\n",
    "    print(col, len(train_df_new[col].unique()))\n",
    "\n",
    "\n",
    "#correlation among features\n",
    "cols_to_use = corr_df.col_labels.tolist()\n",
    "\n",
    "temp_df = df[cols_to_use]\n",
    "corrmat = temp_df.corr(method='spearman')\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(corrmat, vmax=1., square=True)\n",
    "plt.title(\"Important variables correlation map\", fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = df.mean(axis=0)\n",
    "train_df_new = df.fillna(mean_values )\n",
    "x_cols = [col for col in train_df_new.columns if col not in ['target','unique_id'] if 'n' in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pearson correlation\n",
    "# # plot correlation, annmotate with values\n",
    "sns.clustermap(train_df_new[x_cols[:6]].corr(), annot=True, cmap='bwr_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Correlation analysis\n",
    "sns.heatmap(train_df_new[x_cols[:6]].corr(), annot=True, fmt='.2f')\n",
    "correlations = df.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "correlations = correlations[correlations['level_0'] != correlations['level_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = temp_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# upper  #.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# high_corr_var=np.where(corr_matrix>0.8)\n",
    "# high_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "# high_corr_var\n",
    "\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df_new, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_data(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_missing_more_than_80pct = ['N25', 'N26', 'N27', 'N28', 'N29', 'N30', 'N31', 'N32']\n",
    "\n",
    "features_to_drop = to_drop+ corr_zero_cols+var_missing_more_than_80pct + ['Unique_ID']\n",
    "\n",
    "#select only unqiue vars\n",
    "features_to_drop = list(set(features_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering.add_datepart(dfmerged, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_feather('dfv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == 'clf':    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris_data.dropna() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to temporarily drop the rows with 'NA' values\n",
    "# because the Seaborn plotting function does not know\n",
    "# what to do with them\n",
    "sns.pairplot(iris_data.dropna(), hue='class')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We know that we should only have three classes\n",
    "assert len(iris_data_clean['class'].unique()) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We know that sepal lengths for 'Iris-versicolor' should never be below 2.5 cm\n",
    "assert iris_data_clean.loc[iris_data_clean['class'] == 'Iris-versicolor', 'sepal_length_cm'].min() >= 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that our data set should have no missing measurements\n",
    "assert len(iris_data_clean.loc[(iris_data_clean['sepal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['sepal_width_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_width_cm'].isnull())]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate count statistics of duplicate entries\n",
    "if len(df[df.duplicated()]) > 0:\n",
    "    print(\"\\n***Number of duplicated entries: \", len(df[df.duplicated()]))\n",
    "    display(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head())\n",
    "else:\n",
    "    print(\"\\nNo duplicated entries found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather('dfv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Outlier Treatmentment(If you are training linear models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Missing Value Treatmentment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'Saving Account'\n",
    "\n",
    "credit['Saving accounts']= credit['Saving accounts'].map({'little': 'little', 'moderate': 'moderate', 'quite rich':'other','rich':'other', 'NA':'other' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Encode categorical variables\n",
    "#Encoding for target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['categorical_var'] = le.fit_transform(df['categorical_var'])\n",
    "#One hot encoding for categorical information\n",
    "#Use sklearn's OneHotEncoder for categories encoded as possitive real numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "df['var_to_encode'] = enc.fit_transform(df['var_to_encode'])\n",
    "#Use pandas get_dummies for categories encoded as strings\n",
    "pd.get_dummies(df, columns=['col1','col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stole from https://github.com/albertsl/toolkit/blob/master/templates/Data%20Science%20Template.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering. Create new features by transforming the data\n",
    "#Discretize continuous features\n",
    "#Decompose features (categorical, date/time, etc.)\n",
    "#Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.)\n",
    "#Aggregate features into promising new features (x*y)\n",
    "#For speed/movement data, add vectorial features. Try many different combinations\n",
    "df['position_norm'] = df['position_X'] ** 2 + df['position_Y'] ** 2 + df['position_Z'] ** 2\n",
    "df['position_module'] = df['position_norm'] ** 0.5\n",
    "df['position_norm_X'] = df['position_X'] / df['position_module']\n",
    "df['position_norm_Y'] = df['position_Y'] / df['position_module']\n",
    "df['position_norm_Z'] = df['position_Z'] / df['position_module']\n",
    "df['position_over_velocity'] = df['position_module'] / df['velocity_module']\n",
    "#For time series data: Discretize the data by different samples.\n",
    "from astropy.stats import median_absolute_deviation\n",
    "from statsmodels.robust.scale import mad\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "\n",
    "def CPT5(x):\n",
    "    den = len(x)*np.exp(np.std(x))\n",
    "    return sum(np.exp(x))/den\n",
    "\n",
    "def SSC(x):\n",
    "    x = np.array(x)\n",
    "    x = np.append(x[-1], x)\n",
    "    x = np.append(x, x[1])\n",
    "    xn = x[1:len(x)-1]\n",
    "    xn_i2 = x[2:len(x)]    #xn+1\n",
    "    xn_i1 = x[0:len(x)-2]  #xn-1\n",
    "    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2), 0)\n",
    "    return sum(ans[1:])\n",
    "\n",
    "def wave_length(x):\n",
    "    x = np.array(x)\n",
    "    x = np.append(x[-1], x)\n",
    "    x = np.append(x, x[1])\n",
    "    xn = x[1:len(x)-1]\n",
    "    xn_i2 = x[2:len(x)]    #xn+1\n",
    "    return sum(abs(xn_i2-xn))\n",
    "\n",
    "def norm_entropy(x):\n",
    "    tresh = 3\n",
    "    return sum(np.power(abs(x), tresh))\n",
    "\n",
    "def SRAV(x):\n",
    "    SRA = sum(np.sqrt(abs(x)))\n",
    "    return np.power(SRA/len(x), 2)\n",
    "\n",
    "def mean_abs(x):\n",
    "    return sum(abs(x))/len(x)\n",
    "\n",
    "def zero_crossing(x):\n",
    "    x = np.array(x)\n",
    "    x = np.append(x[-1], x)\n",
    "    x = np.append(x, x[1])\n",
    "    xn = x[1:len(x)-1]\n",
    "    xn_i2 = x[2:len(x)]    #xn+1\n",
    "    return sum(np.heaviside(-xn*xn_i2, 0))\n",
    "\n",
    "df_tmp = pd.DataFrame()\n",
    "for column in tqdm(df.columns):\n",
    "    df_tmp[column + '_mean'] = df.groupby(['series_id'])[column].mean()\n",
    "    df_tmp[column + '_median'] = df.groupby(['series_id'])[column].median()\n",
    "    df_tmp[column + '_max'] = df.groupby(['series_id'])[column].max()\n",
    "    df_tmp[column + '_min'] = df.groupby(['series_id'])[column].min()\n",
    "    df_tmp[column + '_std'] = df.groupby(['series_id'])[column].std()\n",
    "    df_tmp[column + '_range'] = df_tmp[column + '_max'] - df_tmp[column + '_min']\n",
    "    df_tmp[column + '_max_over_Min'] = df_tmp[column + '_max'] / df_tmp[column + '_min']\n",
    "    df_tmp[column + 'median_abs_dev'] = df.groupby(['series_id'])[column].mad()\n",
    "    df_tmp[column + '_mean_abs_chg'] = df.groupby(['series_id'])[column].apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "    df_tmp[column + '_mean_change_of_abs_change'] = df.groupby('series_id')[column].apply(lambda x: np.mean(np.diff(np.abs(np.diff(x)))))\n",
    "    df_tmp[column + '_abs_max'] = df.groupby(['series_id'])[column].apply(lambda x: np.max(np.abs(x)))\n",
    "    df_tmp[column + '_abs_min'] = df.groupby(['series_id'])[column].apply(lambda x: np.min(np.abs(x)))\n",
    "    df_tmp[column + '_abs_avg'] = (df_tmp[column + '_abs_min'] + df_tmp[column + '_abs_max'])/2\n",
    "    df_tmp[column + '_abs_mean'] = df.groupby('series_id')[column].apply(lambda x: np.mean(np.abs(x)))\n",
    "    df_tmp[column + '_abs_std'] = df.groupby('series_id')[column].apply(lambda x: np.std(np.abs(x)))\n",
    "    df_tmp[column + '_abs_range'] = df_tmp[column + '_abs_max'] - df_tmp[column + '_abs_min']\n",
    "    df_tmp[column + '_skew'] = df.groupby(['series_id'])[column].skew()\n",
    "    df_tmp[column + '_q25'] = df.groupby(['series_id'])[column].quantile(0.25)\n",
    "    df_tmp[column + '_q75'] = df.groupby(['series_id'])[column].quantile(0.75)\n",
    "    df_tmp[column + '_q95'] = df.groupby(['series_id'])[column].quantile(0.95)\n",
    "    df_tmp[column + '_iqr'] = df_tmp[column + '_q75'] - df_tmp[column + '_q25']\n",
    "    df_tmp[column + '_CPT5'] = df.groupby(['series_id'])[column].apply(CPT5)\n",
    "    df_tmp[column + '_SSC'] = df.groupby(['series_id'])[column].apply(SSC)\n",
    "    df_tmp[column + '_wave_lenght'] = df.groupby(['series_id'])[column].apply(wave_length)\n",
    "    df_tmp[column + '_norm_entropy'] = df.groupby(['series_id'])[column].apply(norm_entropy)\n",
    "    df_tmp[column + '_SRAV'] = df.groupby(['series_id'])[column].apply(SRAV)\n",
    "    df_tmp[column + '_kurtosis'] = df.groupby(['series_id'])[column].apply(kurtosis)\n",
    "    df_tmp[column + '_zero_crossing'] = df.groupby(['series_id'])[column].apply(zero_crossing)\n",
    "    df_tmp[column +  '_unq'] = df[column].round(3).nunique()\n",
    "    try:\n",
    "        df_tmp[column + '_freq'] = df[column].value_counts().idxmax()\n",
    "    except:\n",
    "        df_tmp[column + '_freq'] = 0\n",
    "    df_tmp[column + '_max_freq'] = df[df[column] == df[column].max()].shape[0]\n",
    "    df_tmp[column + '_min_freq'] = df[df[column] == df[column].min()].shape[0]\n",
    "    df_tmp[column + '_pos_freq'] = df[df[column] >= 0].shape[0]\n",
    "    df_tmp[column + '_neg_freq'] = df[df[column] < 0].shape[0]\n",
    "    df_tmp[column + '_nzeros'] = (df[column]==0).sum(axis=0)\n",
    "df = df_tmp.copy()\n",
    "#Create a new column from conditions on other columns\n",
    "df['column_y'] = df[(df['column_x1'] | 'column_x2') & 'column_x3']\n",
    "df['column_y'] = df['column_y'].apply(bool)\n",
    "df['column_y'] = df['column_y'].apply(int)\n",
    "#Create a new True/False column according to the first letter on another column.\n",
    "lEI = [0] * df.shape[0]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        l = df['room_list'].iloc[i].split(', ')\n",
    "    except:\n",
    "        #When the given row is empty\n",
    "        l = []\n",
    "    for element in l:\n",
    "        if element[0] == 'E' or element[0] == 'I':\n",
    "            lEI[i] = 1\n",
    "\n",
    "df['EI'] = pd.Series(lEI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
